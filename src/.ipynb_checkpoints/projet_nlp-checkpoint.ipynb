{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835167ea-cdea-4f97-aa54-53214ae53a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projet : Détection de Fake News avec NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16c6eab-2475-4aeb-97c7-675c5487a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import swifter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                            f1_score, confusion_matrix, roc_curve, auc, \n",
    "                            RocCurveDisplay, ConfusionMatrixDisplay)\n",
    "from tensorflow.keras.models import load_model\n",
    "import shap\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "import os\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ab56b-7715-44f9-bc03-02618f91577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fake_path, true_path):\n",
    "    # Charger le dataset\n",
    "    fake_news = pd.read_csv(fake_path)\n",
    "    true_news = pd.read_csv(true_path)\n",
    "    \n",
    "    # Ajouter les labels\n",
    "    fake_news['label'] = 0  # 0 pour les fausses news\n",
    "    true_news['label'] = 1  # 1 pour les vraies news\n",
    "    \n",
    "    combined_df = pd.concat([true_news, fake_news], axis=0)\n",
    "    combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return combined_df\n",
    "\n",
    "def analyze_data(df):\n",
    "    class_distribution = df['label'].value_counts()\n",
    "    print(\"Distribution des classes:\")\n",
    "    print(class_distribution)\n",
    "\n",
    "    df['text_length'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "    mean_length = df.groupby('label')['text_length'].mean()\n",
    "    print(\"\\nLongueur moyenne des textes (en mots):\")\n",
    "    print(mean_length)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    fake_words = get_top_words(df[df['label'] == 0]['text'])\n",
    "    true_words = get_top_words(df[df['label'] == 1]['text'])\n",
    "    \n",
    "    return fake_words, true_words\n",
    "\n",
    "\n",
    "\n",
    "def get_top_words(text_series, n=20):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    all_words = ' '.join(text_series).split()\n",
    "    filtered_words = [word.lower() for word in all_words if word.lower() not in stop_words and word.isalpha()]\n",
    "    return Counter(filtered_words).most_common(n)\n",
    "\n",
    "\n",
    "\n",
    "def donnee_manquante(df):\n",
    "    print(\"Valeurs manquantes par colonne:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    print(f\"Nombre d'articles avant suppression des doublons: {len(df)}\")\n",
    "    df.drop_duplicates(subset=['text'], keep='first', inplace=True)\n",
    "    print(f\"Nombre d'articles après suppression: {len(df)}\")\n",
    "\n",
    "\n",
    "\n",
    "def unified_text_processor(df, text_col='text'):\n",
    "    # Initialisation des outils NLP\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def clean_and_tokenize(text):\n",
    "        # Nettoyage de base\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+|@\\w+|#\\w+', '', text)\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Tokenization avancée\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Lemmatisation et filtrage\n",
    "        clean_tokens = [\n",
    "            lemmatizer.lemmatize(word) \n",
    "            for word in tokens \n",
    "            if word not in stop_words and len(word) > 2\n",
    "        ]\n",
    "        \n",
    "        return clean_tokens\n",
    "    \n",
    "    # Application principale\n",
    "    df['tokens'] = df[text_col].swifter.apply(clean_and_tokenize)\n",
    "    df['clean_text'] = df['tokens'].apply(' '.join)\n",
    "    \n",
    "    # Features quantitatives\n",
    "    df['word_count'] = df['tokens'].apply(len)\n",
    "    df['char_count'] = df['clean_text'].apply(len)\n",
    "    \n",
    "    # Features linguistiques (sur texte nettoyé)\n",
    "    df['noun_count'] = df['tokens'].apply(\n",
    "        lambda x: sum(1 for _, pos in nltk.pos_tag(x) if pos.startswith('NN'))\n",
    "    )\n",
    "    \n",
    "    # Sentiment analysis\n",
    "    df['sentiment'] = df['clean_text'].swifter.apply(\n",
    "        lambda x: TextBlob(x).sentiment.polarity\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_final_df():\n",
    "\n",
    "    FAKE_PATH = \"../dataSet/Fake.csv\"\n",
    "    TRUE_PATH = \"../dataSet/True.csv\"\n",
    "    print(\"Chargement des données...\")\n",
    "    df = load_data(FAKE_PATH, TRUE_PATH)\n",
    "    print(df.head())\n",
    "\n",
    "    print(\"\\nAnalyse des données...\")\n",
    "    fake_words, true_words = analyze_data(df)\n",
    "    print(\"\\nMots fréquents (Fake):\", fake_words)\n",
    "    print(\"\\nMots fréquents (True):\", true_words)\n",
    "\n",
    "    donnee_manquante(df)\n",
    "\n",
    "\n",
    "    # 4. Traitement NLP complet\n",
    "    print(\"\\nTraitement NLP avancé...\")\n",
    "    final_df = unified_text_processor(df)\n",
    "    print(final_df[['clean_text', 'word_count', 'noun_count', 'sentiment']].head())\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb19c38-2d90-49fe-827f-2c5716ac98f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_final_df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f812e83b-5516-4716-8df4-ff54bafcd2df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec31a4-e9c2-439c-ae48-954dcc014844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(combined_df):\n",
    "    \"\"\"Entraîne un modèle Word2Vec sur les tokens\"\"\"\n",
    "    sentences = combined_df['tokens'].tolist()\n",
    "    \n",
    "    model =  Word2Vec(\n",
    "        sentences,\n",
    "        vector_size=300,\n",
    "        window=10,\n",
    "        min_count=3,\n",
    "        negative=10,\n",
    "        hs=1,\n",
    "        sample=1e-5,\n",
    "        workers=8,\n",
    "        epochs=20\n",
    ")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_embeddings(model, combined_df):\n",
    "    \"\"\"Crée des embeddings moyens pour chaque document\"\"\"\n",
    "    def document_vector(tokens):\n",
    "        words = [word for word in tokens if word in model.wv]\n",
    "        return np.mean(model.wv[words], axis=0) if words else np.zeros(model.vector_size)\n",
    "    \n",
    "    combined_df['w2v_embedding'] = combined_df['tokens'].apply(document_vector)\n",
    "    return combined_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97c7bc0-adfa-4b04-b92e-4048d9e0fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_final_df()  # Appel direct à votre fonction de data.py\n",
    "    \n",
    "# 2. Entraînement du modèle\n",
    "w2v_model = train_word2vec(df)\n",
    "\n",
    "# 3. Création des embeddings\n",
    "df_with_embeddings = create_embeddings(w2v_model, df)\n",
    "\n",
    "# 4. Sauvegarde\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "w2v_model.save(\"models/w2v_fake_news.model\")\n",
    "df_with_embeddings.to_pickle(\"processed_data/news_with_embeddings.pkl\")\n",
    "\n",
    "print(\"Modèle Word2Vec et embeddings créés avec succès!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab92c7e-497d-4756-9860-83e958b02582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46102732-8d90-4fa8-9601-be32dd9add38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3e7f5-2f5d-48fc-8852-778e52b70cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbddfc9-06fe-4552-bbba-60166e7b126a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e604f454-7c18-4eeb-b704-2a8b23ef0f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68de09a-2947-4275-ad0e-2878f7802249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
